{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82405714",
   "metadata": {},
   "source": [
    "## Used\n",
    "To load file use:\n",
    "\n",
    "`%load filename.py`\n",
    "\n",
    "To write file use:\n",
    "\n",
    "`%%writefile filename.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c1db93e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting run.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile run.py\n",
    "# from ocr import ocr\n",
    "from pipeline import app\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=True, use_reloader=True)\n",
    "#OKE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9412621d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile processing_tools.py\n",
    "import cv2\n",
    "import grpc\n",
    "from tensorflow_serving.apis import predict_pb2\n",
    "from tensorflow_serving.apis import prediction_service_pb2_grpc\n",
    "import numpy as np\n",
    "import urllib\n",
    "\n",
    "\n",
    "def url_to_image(url):\n",
    "    \"\"\"\n",
    "    Read image from url\n",
    "    \"\"\"\n",
    "    # resp = urllib.request.urlopen(url)\n",
    "    # image = np.asarray(bytearray(resp.read()), dtype=\"uint8\")\n",
    "    # image = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
    "    image = cv2.imread(url)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    return image\n",
    "\n",
    "def get_parameters_from_model_url(url):\n",
    "    \"\"\"\n",
    "    Get parameters from a model url.\n",
    "\n",
    "    Parameters:\n",
    "        url: str, url of the model\n",
    "\n",
    "    Returns:\n",
    "        dict, parameters of the model\n",
    "    \"\"\"\n",
    "    params = {}\n",
    "\n",
    "    netloc = url.split(\"//\")[1].split(\"/\")[0]\n",
    "    params['netloc'] = netloc\n",
    "\n",
    "    url = url.split(\"?\")[1]\n",
    "    url = url.split(\"&\")\n",
    "    for param in url:\n",
    "        key, value = param.split(\"=\")\n",
    "        params[key] = value\n",
    "    return params\n",
    "\n",
    "def get_grpc_predict(url, input_name, input):\n",
    "    \"\"\"\n",
    "    Get grpc predict stub\n",
    "    \n",
    "    Parameters:\n",
    "        url: string, url of the server\n",
    "        input_name: string, input name of the model\n",
    "        input: numpy array, input data\n",
    "\n",
    "    Returns:\n",
    "        predict_stub: grpc stub\n",
    "    \"\"\"\n",
    "    params = get_parameters_from_model_url(url)\n",
    "    SERVER = params['netloc']\n",
    "    GRPC_MAX_RECEIVE_MESSAGE_LENGTH = 4096 * 4096 * 3\n",
    "\n",
    "    channel = grpc.insecure_channel(SERVER, options=[('grpc.max_receive_message_length', GRPC_MAX_RECEIVE_MESSAGE_LENGTH)])\n",
    "    stub = prediction_service_pb2_grpc.PredictionServiceStub(channel)\n",
    "    request = predict_pb2.PredictRequest()\n",
    "\n",
    "    request.model_spec.name = params['model_name']\n",
    "    request.model_spec.signature_name = 'serving_default'\n",
    "    if params['version'] is not None:\n",
    "        request.model_spec.version.value = int(params['version'])\n",
    "\n",
    "    request.inputs[input_name].CopyFrom(input)\n",
    "    result = stub.Predict(request, 10.0)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57832942",
   "metadata": {},
   "source": [
    "## Face matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fe64e89b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pipeline/face_matching/mtcnn.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile pipeline/face_matching/mtcnn.py\n",
    "import time\n",
    "import requests\n",
    "import tensorflow as tf\n",
    "from processing_tools import get_grpc_predict\n",
    "from pipeline.face_matching.box_utils import calibrate_box, convert_to_square, get_image_boxes, generate_bboxes, preprocess\n",
    "tf.config.run_functions_eagerly(run_eagerly=True)\n",
    "\n",
    "DEF_THRESHOLDS = [0.6, 0.7, 0.7]\n",
    "DEF_NMS_THRESHOLDS = [0.6, 0.6, 0.6]\n",
    "\n",
    "class MTCNN(object):\n",
    "    \"\"\" Top level class for mtcnn detection \"\"\"\n",
    "    def __init__(self, pnet_url, rnet_url, onet_url,\n",
    "                 min_face_size=20.0,\n",
    "                 thresholds=None,\n",
    "                 nms_thresholds=None,\n",
    "                 max_output_size=300):\n",
    "        self.pnet_url = pnet_url\n",
    "        self.rnet_url = rnet_url\n",
    "        self.onet_url = onet_url\n",
    "        self.min_face_size = min_face_size\n",
    "        self.thresholds = thresholds or DEF_THRESHOLDS\n",
    "        self.nms_thresholds = nms_thresholds or DEF_NMS_THRESHOLDS\n",
    "        self.max_output_size = max_output_size\n",
    "        self.scale_cache = {}\n",
    "\n",
    "    def detect(self, img):\n",
    "        \"\"\"Detect faces and facial landmarks on an image\n",
    "\n",
    "        Parameters:\n",
    "            img: rgb image, numpy array of shape [h, w, 3]\n",
    "\n",
    "        Returns:\n",
    "            bboxes: float tensor of shape [n, 4], face bounding boxes\n",
    "            landmarks: float tensor of shape [n, 10], 5 facial landmarks,\n",
    "                        first 5 numbers of array are x coords, last are y coords\n",
    "            scores: float tensor of shape [n], confidence scores\n",
    "        \"\"\"\n",
    "        height, width, _ = img.shape\n",
    "        img = tf.convert_to_tensor(img, tf.float32)\n",
    "        scales = self.get_scales(height, width)\n",
    "        start = time.time()\n",
    "        bboxes = self.stage_one(img, scales)\n",
    "        end = time.time()\n",
    "        print('stage one: {}s'.format(end - start))\n",
    "        if len(bboxes) == 0:\n",
    "            return [], [], []\n",
    "        bboxes = self.stage_two(img, bboxes, height, width, bboxes.shape[0])\n",
    "        if len(bboxes) == 0:\n",
    "            return [], [], []\n",
    "        bboxes, landmarks, scores = self.stage_three(img, bboxes,\n",
    "                                                     height, width, bboxes.shape[0])\n",
    "        return bboxes\n",
    "\n",
    "    def get_scales(self, height, width):\n",
    "        \"\"\"Compute scaling factors for given image dimensions\n",
    "\n",
    "        Parameters:\n",
    "            height: float\n",
    "            width: float\n",
    "\n",
    "        Returns:\n",
    "            list of floats, scaling factors\n",
    "        \"\"\"\n",
    "        min_length = min(height, width)\n",
    "        # typically scaling factors will not change in a video feed\n",
    "        if min_length in self.scale_cache:\n",
    "            return self.scale_cache[min_length]\n",
    "\n",
    "        min_detection_size = 12\n",
    "        factor = 0.707  # sqrt(0.5)\n",
    "        # scales for scaling the image\n",
    "        scales = []\n",
    "        # scales the image so that\n",
    "        # minimum size that we can detect equals to\n",
    "        # minimum face size that we want to detect\n",
    "        m = min_detection_size / self.min_face_size\n",
    "        min_length *= m\n",
    "        factor_count = 0\n",
    "        while min_length > min_detection_size:\n",
    "            scales.append(m * factor**factor_count)\n",
    "            min_length *= factor\n",
    "            factor_count += 1\n",
    "\n",
    "        self.scale_cache[min_length] = scales\n",
    "        return scales\n",
    "\n",
    "    # @tf.function(\n",
    "    #     input_signature=[tf.TensorSpec(shape=(None, None, 3), dtype=tf.float32),\n",
    "    #                      tf.TensorSpec(shape=(), dtype=tf.float32),\n",
    "    #                      tf.TensorSpec(shape=(), dtype=tf.float32),\n",
    "    #                      tf.TensorSpec(shape=(), dtype=tf.float32)])\n",
    "    def stage_one_scale(self, img, height, width, scale):\n",
    "        \"\"\"Perform stage one part with a given scaling factor\n",
    "\n",
    "        Parameters:\n",
    "            img: rgb image, float tensor of shape [h, w, 3]\n",
    "            height: image height, float\n",
    "            width: image width, float\n",
    "            scale: scaling factor, float\n",
    "\n",
    "        Returns:\n",
    "            float tensor of shape [n, 9]\n",
    "        \"\"\"\n",
    "        hs = tf.math.ceil(height * scale)\n",
    "        ws = tf.math.ceil(width * scale)\n",
    "        img_in = tf.image.resize(img, (hs, ws))\n",
    "        img_in = preprocess(img_in)\n",
    "        img_in = tf.expand_dims(img_in, 0)\n",
    "        img_in = tf.make_tensor_proto(img_in)\n",
    "        # img_in = tf.make_ndarray(img_in)\n",
    "\n",
    "        # payload = {'instances': img_in.tolist()}\n",
    "        # res = requests.post(self.pnet_url, json=payload)\n",
    "        # res = res.json()['predictions'][0]\n",
    "        result = get_grpc_predict(self.pnet_url, 'input_3', img_in)\n",
    "        \n",
    "        conv2d_12 = tf.make_ndarray(result.outputs['conv2d_12'])\n",
    "        softmax_2 = tf.make_ndarray(result.outputs['softmax_2'])\n",
    "        # print(softmax_2.shape)\n",
    "        # print(conv2d_12.shape)\n",
    "        # probs = softmax_2[0]\n",
    "        # offsets = conv2d_12[0]\n",
    "\n",
    "        probs = tf.convert_to_tensor(softmax_2[0])\n",
    "        offsets = tf.convert_to_tensor(conv2d_12[0])\n",
    "        # probs = tf.convert_to_tensor(res['softmax_2'])\n",
    "        # offsets = tf.convert_to_tensor(res['conv2d_12'])\n",
    "\n",
    "        boxes = generate_bboxes(probs, offsets, scale, self.thresholds[0])\n",
    "\n",
    "        if len(boxes) == 0:\n",
    "            return boxes\n",
    "        keep = tf.image.non_max_suppression(boxes[:, :4], boxes[:, 4], self.max_output_size,\n",
    "                                            iou_threshold=0.5)\n",
    "\n",
    "        boxes = tf.gather(boxes, keep)\n",
    "        return boxes\n",
    "\n",
    "    # @tf.function(\n",
    "    #     input_signature=[tf.TensorSpec(shape=(None, 9), dtype=tf.float32)])\n",
    "    def stage_one_filter(self, boxes):\n",
    "        \"\"\"Filter out boxes in stage one \n",
    "\n",
    "        Parameters:\n",
    "            boxes: collected boxes with different scales, float tensor of shape [n, 9]\n",
    "\n",
    "        Returns:\n",
    "            float tensor of shape [n, 4]\n",
    "        \"\"\"\n",
    "        bboxes, scores, offsets = boxes[:, :4], boxes[:, 4], boxes[:, 5:]\n",
    "        # use offsets predicted by pnet to transform bounding boxes\n",
    "        bboxes = calibrate_box(bboxes, offsets)\n",
    "        bboxes = convert_to_square(bboxes)\n",
    "\n",
    "        keep = tf.image.non_max_suppression(bboxes, scores, self.max_output_size,\n",
    "                                            iou_threshold=self.nms_thresholds[0])\n",
    "        bboxes = tf.gather(bboxes, keep)\n",
    "        return bboxes\n",
    "\n",
    "    def stage_one(self, img, scales):\n",
    "        \"\"\"Run stage one on the input image\n",
    "\n",
    "        Parameters:\n",
    "            img: rgb image, float tensor of shape [h, w, 3]\n",
    "            scales: scaling factors, list of floats\n",
    "\n",
    "        Returns:\n",
    "            float tensor of shape [n, 4], predicted bounding boxes\n",
    "        \"\"\"\n",
    "        height, width, _ = img.shape\n",
    "        boxes = []\n",
    "\n",
    "        # run P-Net on different scales\n",
    "        for s in scales:\n",
    "            boxes.append(self.stage_one_scale(img, height, width, s))\n",
    "        # collect boxes (and offsets, and scores) from different scales\n",
    "        boxes = tf.concat(boxes, 0)\n",
    "        if boxes.shape[0] == 0:\n",
    "            return []\n",
    "        return self.stage_one_filter(boxes)\n",
    "\n",
    "    # @tf.function(\n",
    "    #     input_signature=[tf.TensorSpec(shape=(None, None, 3), dtype=tf.float32),\n",
    "    #                      tf.TensorSpec(shape=(None, 4), dtype=tf.float32),\n",
    "    #                      tf.TensorSpec(shape=(), dtype=tf.float32),\n",
    "    #                      tf.TensorSpec(shape=(), dtype=tf.float32),\n",
    "    #                      tf.TensorSpec(shape=(), dtype=tf.int32)])\n",
    "    def stage_two(self, img, bboxes, height, width, num_boxes):\n",
    "        \"\"\"Run stage two on the input image\n",
    "\n",
    "        Parameters:\n",
    "            img: rgb image, float tensor of shape [h, w, 3]\n",
    "            bboxes: bounding boxes from stage one, float tensor of shape [n, 4]\n",
    "            height: image height, float\n",
    "            width: image width, float\n",
    "            num_boxes: number of rows in bboxes, int\n",
    "\n",
    "        Returns:\n",
    "            float tensor of shape [n, 4], predicted bounding boxes\n",
    "        \"\"\"\n",
    "        img_boxes = get_image_boxes(bboxes, img, height, width, num_boxes, size=24)\n",
    "        img_in = tf.make_tensor_proto(img_boxes)\n",
    "        # img_in = tf.make_ndarray(img_in)\n",
    "\n",
    "        # payload = {'instances': img_in.tolist()}\n",
    "        # res = requests.post(self.rnet_url, json=payload)\n",
    "        # res = res.json()['predictions']\n",
    "        result = get_grpc_predict(self.rnet_url, 'input_1', img_in)\n",
    "\n",
    "        dense5_2 = tf.make_ndarray(result.outputs['dense5_2'])\n",
    "        softmax = tf.make_ndarray(result.outputs['softmax'])\n",
    "        probs = tf.convert_to_tensor(softmax)\n",
    "        offsets = tf.convert_to_tensor(dense5_2)\n",
    "        # for i in range(len(res)):\n",
    "        #     probs.append(res[i]['softmax'])\n",
    "        #     offsets.append(res[i]['dense5_2'])\n",
    "\n",
    "        # probs = tf.convert_to_tensor(probs)\n",
    "        # offsets = tf.convert_to_tensor(offsets)\n",
    "\n",
    "        keep = tf.where(probs[:, 1] > self.thresholds[1])[:, 0]\n",
    "\n",
    "        bboxes = tf.gather(bboxes, keep)\n",
    "        offsets = tf.gather(offsets, keep)\n",
    "        scores = tf.gather(probs[:, 1], keep)\n",
    "\n",
    "        bboxes = calibrate_box(bboxes, offsets)\n",
    "        bboxes = convert_to_square(bboxes)\n",
    "\n",
    "        keep = tf.image.non_max_suppression(bboxes, scores,\n",
    "                                            self.max_output_size, self.nms_thresholds[1])\n",
    "        bboxes = tf.gather(bboxes, keep)\n",
    "        return bboxes\n",
    "\n",
    "    # @tf.function(\n",
    "    #     input_signature=[tf.TensorSpec(shape=(None, None, 3), dtype=tf.float32),\n",
    "    #                      tf.TensorSpec(shape=(None, 4), dtype=tf.float32),\n",
    "    #                      tf.TensorSpec(shape=(), dtype=tf.float32),\n",
    "    #                      tf.TensorSpec(shape=(), dtype=tf.float32),\n",
    "    #                      tf.TensorSpec(shape=(), dtype=tf.int32)])\n",
    "    def stage_three(self, img, bboxes, height, width, num_boxes):\n",
    "        \"\"\"Run stage three on the input image\n",
    "\n",
    "        Parameters:\n",
    "            img: rgb image, float tensor of shape [h, w, 3]\n",
    "            bboxes: bounding boxes from stage two, float tensor of shape [n, 4]\n",
    "            height: image height, float\n",
    "            width: image width, float\n",
    "            num_boxes: number of rows in bboxes, int\n",
    "\n",
    "        Returns:\n",
    "            bboxes: float tensor of shape [n, 4], face bounding boxes\n",
    "            landmarks: float tensor of shape [n, 10], 5 facial landmarks,\n",
    "                        first 5 numbers of array are x coords, last are y coords\n",
    "            scores: float tensor of shape [n], confidence scores\n",
    "        \"\"\"\n",
    "        img_boxes = get_image_boxes(bboxes, img, height, width, num_boxes, size=48)\n",
    "        img_boxes = tf.make_tensor_proto(img_boxes)\n",
    "        # img_boxes = tf.make_ndarray(img_boxes)\n",
    "        # payload = {'instances': img_boxes.tolist()}\n",
    "        # res = requests.post(self.onet_url, json=payload)\n",
    "        # res = res.json()['predictions']\n",
    "        result = get_grpc_predict(self.onet_url, 'input_2', img_boxes)\n",
    "\n",
    "        dense6_2 = tf.make_ndarray(result.outputs['dense6_2'])\n",
    "        softmax_1 = tf.make_ndarray(result.outputs['softmax_1'])\n",
    "        dense6_3 = tf.make_ndarray(result.outputs['dense6_3'])\n",
    "        probs = tf.convert_to_tensor(softmax_1)\n",
    "        offsets = tf.convert_to_tensor(dense6_2)\n",
    "        landmarks = tf.convert_to_tensor(dense6_3)\n",
    "        # probs = []\n",
    "        # offsets = []\n",
    "        # landmarks = []\n",
    "        # for i in range(len(res)):\n",
    "        #     probs.append(res[i]['softmax_1'])\n",
    "        #     offsets.append(res[i]['dense6_2'])\n",
    "        #     landmarks.append(res[i]['dense6_3'])\n",
    "        # probs = tf.convert_to_tensor(probs)\n",
    "        # offsets = tf.convert_to_tensor(offsets)\n",
    "        # landmarks = tf.convert_to_tensor(landmarks)\n",
    "        keep = tf.where(probs[:, 1] > self.thresholds[2])[:, 0]\n",
    "        bboxes = tf.gather(bboxes, keep)\n",
    "        offsets = tf.gather(offsets, keep)\n",
    "        scores = tf.gather(probs[:, 1], keep)\n",
    "        landmarks = tf.gather(landmarks, keep)\n",
    "\n",
    "        # compute landmark points\n",
    "        width = tf.expand_dims(bboxes[:, 2] - bboxes[:, 0] + 1.0, 1)\n",
    "        height = tf.expand_dims(bboxes[:, 3] - bboxes[:, 1] + 1.0, 1)\n",
    "        xmin = tf.expand_dims(bboxes[:, 0], 1)\n",
    "        ymin = tf.expand_dims(bboxes[:, 1], 1)\n",
    "        landmarks = tf.concat([xmin + width * landmarks[:, 0:5],\n",
    "                               ymin + height * landmarks[:, 5:10]], 1)\n",
    "\n",
    "        bboxes = calibrate_box(bboxes, offsets)\n",
    "        keep = tf.image.non_max_suppression(bboxes, scores,\n",
    "                                            self.max_output_size, self.nms_thresholds[2])\n",
    "        bboxes = tf.gather(bboxes, keep)\n",
    "        landmarks = tf.gather(landmarks, keep)\n",
    "        scores = tf.gather(scores, keep)\n",
    "        return bboxes, landmarks, scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79485859",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0724d0a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
